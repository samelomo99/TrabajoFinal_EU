mutate(
Pobre_pred = factor(Pobre_pred, levels = c(0, 1)),
Pobre = factor(Pobre, levels = c("No", "Si"), labels = c(0,1))  # Aseguramos que Pobre también sea factor
)
# Matriz de confusión
confusionMatrix(eval_cvrf$Pobre_pred, eval_cvrf$Pobre, positive = "1")
# Armamos el data frame final con la predicción
predict_CVRF_final <- test_split %>%
dplyr::select(id) %>%
mutate(prob_Si = phat_CVRF,
pobre_lab = ifelse(prob_Si >= 0.3, "Si", "No"),
Pobre_pred = ifelse(pobre_lab == "Si", 1, 0)) %>%
dplyr::select(id, Pobre_pred)
head(predict_CVRF_final)
# Unimos con la variable real
eval_cvrf <- test_split %>%
dplyr::select(id, Pobre) %>%
left_join(predict_CVRF_final, by = "id")
unique(eval_cvrf$Pobre)
unique(eval_cvrf$Pobre_pred)
# Convertimos a factor para la matriz de confusión
eval_cvrf <- eval_cvrf %>%
mutate(
Pobre_pred = factor(Pobre_pred, levels = c(0, 1)),
Pobre = factor(Pobre, levels = c("No", "Si"), labels = c(0,1))  # Aseguramos que Pobre también sea factor
)
# Matriz de confusión
confusionMatrix(eval_cvrf$Pobre_pred, eval_cvrf$Pobre, positive = "1")
cv_RForest
# Matriz de confusión
confusionMatrix(eval_cvrf$Pobre_pred, eval_cvrf$Pobre, positive = "1")
randomForest::varImpPlot(cv_RForest)
cv_RForest
randomForest::varImpPlot(cv_RForest)
# Hacemos la predicción
predict_CV_RForest <- test %>%
mutate(prob_Si = predict(cv_RForest, newdata = test, type = "prob")[, "Si"],
pobre_lab = ifelse(prob_Si >= 0.3, "Si", "No"))
# Convertimos la etiqueta (pobre_lab) a formato binario (1 para "Si", 0 para "No")
predict_CV_RForest <- predict_CV_RForest %>%
mutate(pobre = ifelse(pobre_lab == "Si", 1, 0)) %>%
dplyr::select(id, pobre)
head(predict_CV_RForest)
randomForest::varImpPlot(predict_CV_RForest)
cv_RForest
randomForest::varImpPlot(cv_RForest)
varImp(cv_RForest)
# Para encontrar la importancia de las variables sin REPETIR TODO EL MODELO DE NUEVO DEBIDO A LA ESPECIFICACIÓN DE TRAIN DE CARET
library(ranger)
modelo_importancia <- ranger(Pobre ~ .,
data = train_rf[sample(nrow(train_rf), 5000), ],
importance = "impurity",
num.trees = 100)
modelo_importancia$variable.importance
library(ggplot2)
# Crear un dataframe con las importancias
importancia_df <- data.frame(
Variable = names(modelo_importancia$variable.importance),
Importancia = modelo_importancia$variable.importance
)
# Ordenar las importancias de mayor a menor
importancia_df <- importancia_df[order(importancia_df$Importancia, decreasing = TRUE), ]
# Graficar las importancias
ggplot(importancia_df, aes(x = reorder(Variable, Importancia), y = Importancia)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
labs(title = "Importancia de Variables - Modelo Random Forest",
x = "Variable", y = "Importancia") +
theme_minimal()
## Módulo 7
# Datos Espaciales en R
require("pacman")
#Cargamos sf que será la librería a utilizar y tidyverse para nuestras necesidades de manejo de datos.
p_load(sf, tidyverse)
## Visualización estática
#Cargamos los datos de los colegios.
colegios <- read_sf("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/1585e2fef4eb7189af500f52a1022073d3ab0de0/colegios_bogota.json")
## Visualización estática
#Cargamos los datos de los colegios.
colegios <- read_sf("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/1585e2fef4eb7189af500f52a1022073d3ab0de0/colegios_bogota.json")
head(colegios)
class(colegios)
View(colegios)
ggplot()+
geom_sf(data=colegios)
ggplot()+
geom_sf(data=colegios) +
theme_bw()
# -- LINEAS
# Ciclovía, datos en shapefile; descargarlos desde github, descomprimirlos y colocarlos en una carpeta
# llamada datos
ciclovias <- st_read("C:\Users\samel\OneDrive\Datos adjuntos\Universidad de los Andes\IV\Big Data - Machine Learning\Módulo 7\Datos\Ciclovia")
# -- LINEAS
# Ciclovía, datos en shapefile; descargarlos desde github, descomprimirlos y colocarlos en una carpeta
# llamada datos
ciclovias <- st_read("C:/Users/samel/OneDrive/Datos adjuntos/Universidad de los Andes/IV/Big Data - Machine Learning/Módulo 7/Datos/Ciclovia")
head(ciclovias)
ggplot()+
geom_sf(data=ciclovias) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
upla <- st_read("C:/Users/samel/OneDrive/Datos adjuntos/Universidad de los Andes/IV/Big Data - Machine Learning/Módulo 7/Datos/UPLA/upla")
ggplot()+
geom_sf(data=upla) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
ggplot()+
geom_sf(data=upla, aes(fill = UPlArea)) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
# -- TODO AL TIEMPO
ggplot()+
geom_sf(data=upla %>% filter(grepl("RIO",UPlNombre)==FALSE),size=.3,fill=NA) + #filtramos usando expresiones regulares
geom_sf(data=colegios[1:250,],shape=18) + #utilizamos solo los primeros 250 colegios y los representamos con romboides (shape=18)
geom_sf(data=ciclovias,col="red") +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
# Para revisar en qué proyección están los datos, usamos la siguiente función
st_crs(ciclovias)
st_crs(colegios) #
# Para poder graficar y trabajar con estas bases es importante siempre verificar la
# proyección en la que se encuentran y homogeneizarla. Para esto se puede hacer lo
# siguiente.
clovias<-st_transform(ciclovias,4686)
st_crs(ciclovias)
st_crs(clovias)
st_crs(colegios) # Según está en MAGNA - SIRGAS
st_crs(clovias)
st_crs(colegios) # Según está en MAGNA - SIRGAS
## Midiendo distancias
# Generamos el data.frame que tiene columnas: lugar, latitud y longitud, y una columna
# que se llama nudge que sirve para gráficar.
db<-data.frame(place=c("Uniandes","Banco de La Republica"),lat=c(4.601590,4.602151), long=c(-74.066391,-74.072350), nudge_y=c(-0.001,0.001))
db<-db %>% mutate(latp=lat,longp=long)
# Lo transformamos a sf especificando la geometría (puntos)
db<-st_as_sf(db,coords=c('longp','latp'),crs=4326)
db
st_crs(db)
# Definimos la proyección WGS84, ya que tenemos latitudes y longitudes que lo ubican
# en el mapa, pero luego lo proyectamos a MAGNA-SIRGAS para tener la proyección que
# corresponde a Bogotá.
db<-st_transform(db,4686)
st_crs(db)
# Antes de calcular la distancia, graficamos las ubicaciones en sus respectivas UPLs.
ggplot()+
geom_sf(data=upla %>% filter(UPlNombre%in%c("LA CANDELARIA","LAS NIEVES")), fill = NA) +
geom_sf(data=db, col="red") +
geom_label(data = db, aes(x = long, y = lat, label = place),
size = 3, col = "black", fontface = "bold", nudge_y =db$nudge_y) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
# Para calcular la distancia entre Uniandes y Banrep vamos a hacer lo siguiente
st_distance(db)
# El cálculo de distancias también se puede hacer entre puntos y líneas. Por ejemplo,
# usando las ciclovías.
st_distance(db,ciclovias)
# El cálculo de distancias también se puede hacer entre puntos y líneas. Por ejemplo,
# usando las ciclovías.
# Para eso es importante que los objetos espaciales estén en la misma proyección.
ciclovias <- st_transform(ciclovias, 4686)
db <- st_transform(db, 4686)
st_distance(db,ciclovias)
ggplot()+
geom_sf(data=ciclovias[8,], fill = NA) +
geom_sf(data=db, col="red") +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
## Uniones espaciales
upla<-st_transform(upla,crs=st_crs(db))
db<- st_join(db,upla,join=st_intersects)
head(db)
# Cargamos librerías
# Cargamos librerias
require("pacman")
p_load(tidyverse, sf, tmaptools)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá")
p_load(tidyverse, sf, tmaptools)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá")
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá")
install.packages("tidygeocoder")
library(tidygeocoder)
geocode("Casa de Nariño, Bogotá", method = "osm", verbose = TRUE) # Alternativa ChatGpt
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá")
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá", as.sf = TRUE)
p_load(tidyverse, sf, tmaptools)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá", as.sf = TRUE)
# Cargamos librerías
# Cargamos librerias
require("pacman")
p_load(tidyverse, sf, tmaptools)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá", as.sf = TRUE)
geocode_OSM("Casa Rosada, Buenos Aires",as.sf=TRUE)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogota", as.sf = TRUE)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Nariño, Bogotá", as.sf = TRUE)
# Buscamos la coordenada de la Casa de Nariño en Bogotá
geocode_OSM("Casa de Narino, Bogotá", as.sf = TRUE)
geocode_OSM("Casa Rosada, Buenos Aires",as.sf=TRUE)
# Cargamos osmdata
p_load("osmdata")
available_features() %>% head(20)
# Cada feature contiene una lsita de tags, que describen un atributo geográfico
available_tags("amenity")
# Para obtener estos datos primero se debe definir la ubicación geográfica de donde
# queremos estos objetos. Por ejemplo, si queremos para Bogotá debemos definir el
# rectángulo que delimita geográficamente a la ciudad. Para eso hacemos lo siguiente:
bogota <- opq(bbox = getbb("Bogotá Colombia"))
bogota
# Tras definir el lugar, podemos descargar el objeto que deseemos.
# Por ejemplo, universidades.
universidades <- bogota %>% add_osm_feature(key = "amenity", value = "university") %>%
osmdata_sf() # transformamos a un objeto sf
puntos_universidades <- universidades$osm_points
head(puntos_universidades)
ggplot()+
geom_sf(data=puntos_universidades) +
theme_bw()
# En ciertos casos también se tienen los polígonos de los edificios
ML<-universidades$osm_polygons %>% filter(grepl("Mario Laserna" ,name))
ML
ggplot()+
geom_sf(data=ML) +
theme_bw()
# -- VISUALIZACIONES INTERACTIVAS
# Librería leaflet para generar visualizaciones interactivas
#importamos la librería
p_load("leaflet")
# Se requiere especificar primero una capa base, y luego las capas deseadas.
leaflet() %>%
addTiles() %>%  #capa base
addPolygons(data=ML) #capa edificio ML
# Hay muchas capas base disponibles para leaflet. Por ejemplo, Stamen.Toner
leaflet() %>%
addProviderTiles(providers$Stamen.Toner) %>%  #capa base
addCircles(data=puntos_universidades, popup = ~name) %>%  #agregamos puntos con un `pop up` que indica el nombre
addPolygons(data=ML) #capa edificio ML
# Hay muchas capas base disponibles para leaflet. Por ejemplo, Stamen.Toner
leaflet() %>%
addProviderTiles(providers$Stamen.Toner) %>%  #capa base
addCircles(data=puntos_universidades, popup = ~name) %>%  #agregamos puntos con un `pop up` que indica el nombre
addPolygons(data=ML) #capa edificio ML
library(recipes)
library(pacman)
pacman::p_load(
readr,        # Importar datos
labelled,     # Manejo de etiquetas
naniar,       # Visualizar datos faltantes
DataExplorer, # Gráficos de missing values
psych,        # Estadísticas descriptivas
rvest,        # Web scraping
rio,          # Importar/exportar datos
tidyverse,    # Conjunto de paquetes tidy data
skimr,        # Resumen de datos
visdat,       # Visualizar datos faltantes
corrplot,     # Gráficos de correlación
gridExtra,    # Organización de gráficos
MASS,         # Funciones estadísticas diversas
stargazer,    # Tablas para salida a TEX
chromote,     # Automatización de navegador
ggplot2,      # Gráficos
boot,         # Funciones de bootstrap
patchwork,    # Combinación de gráficos
caret,        # Evaluación de modelos
purrr,        # Funciones map_*
kableExtra,   # Opciones de tablas
dplyr,        # Manipulación de datos
summarytools, # Descriptivos enriquecidos
knitr,        # kable en LaTeX
xtable,       # Tablas en LaTeX
tidyr,        # Separar/unir columnas
gmodels,      # CrossTable y más
glmnet,       # Regularización Lasso/Ridge
ranger,       # Random Forest rápido
randomForest, # Random Forest clásico
Metrics,      # Métricas de evaluación
adabag,       # Bagging y boosting
rsample,      # Remuestreo
rpart,        # Árboles de decisión
rpart.plot,   # Gráficos para árboles
ipred,        # Modelos ensamblados
gbm,          # Boosting
stringi,      # Manipulación de texto
sf,           # Datos espaciales
tidymodels,   # Framework de modelado
spatialsample,# Muestreo espacial
plotly,       # Gráficos interactivos
leaflet,      # Mapas interactivos
tmaptools,    # Herramientas espaciales
osmdata,      # Datos OpenStreetMap
tm,           # Text Mining
tidytext,     # Procesamiento de texto
stopwords,    # Stopwords multilingües
parsnip,      # Especificación de modelos
dials,        # Hiperparámetros
workflows,    # Pipelines
tune,         # Tuning de modelos
yardstick,    # Métricas
udpipe,       # Procesamiento NLP
stringr,      # Manipulación de cadenas
nnet          # Redes neuronales simples
)
# Revisamos MAE y MSE
# Calculamos el RMSE y MAE de las predicciones del modelo 1
rmse_1 <- test_split %>%
bind_cols(predictiones_1) %>%
yardstick::rmse(truth = price, estimate = .pred)
# MAE
mae_rf_1 <- yardstick::mae(bind_cols(test_split, pred_rf_1), truth = price, estimate = .pred)
# 8. Predecir sobre test_split y calcular RMSE ----
pred_rf_1 <- predict(fit_rf_1, new_data = test_split)
pred_rf_2 <- predict(fit_rf_2, new_data = test_split)
### SUPERLEARNER ----
# 1. Cargar librerías
library(SuperLearner)
library(tidyverse)
install.packages("recipes")
if (!requireNamespace("recipes", quietly = TRUE)) {
install.packages("recipes")
}
install.packages("recipes")
library(recipes)
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(
readr,        # Importar datos
labelled,     # Manejo de etiquetas
naniar,       # Visualizar datos faltantes
DataExplorer, # Gráficos de missing values
psych,        # Estadísticas descriptivas
rvest,        # Web scraping
rio,          # Importar/exportar datos
tidyverse,    # Conjunto de paquetes tidy data
skimr,        # Resumen de datos
visdat,       # Visualizar datos faltantes
corrplot,     # Gráficos de correlación
gridExtra,    # Organización de gráficos
MASS,         # Funciones estadísticas diversas
stargazer,    # Tablas para salida a TEX
chromote,     # Automatización de navegador
ggplot2,      # Gráficos
boot,         # Funciones de bootstrap
patchwork,    # Combinación de gráficos
caret,        # Evaluación de modelos
purrr,        # Funciones map_*
kableExtra,   # Opciones de tablas
dplyr,        # Manipulación de datos
summarytools, # Descriptivos enriquecidos
knitr,        # kable en LaTeX
xtable,       # Tablas en LaTeX
tidyr,        # Separar/unir columnas
gmodels,      # CrossTable y más
glmnet,       # Regularización Lasso/Ridge
ranger,       # Random Forest rápido
randomForest, # Random Forest clásico
Metrics,      # Métricas de evaluación
adabag,       # Bagging y boosting
rsample,      # Remuestreo
rpart,        # Árboles de decisión
rpart.plot,   # Gráficos para árboles
ipred,        # Modelos ensamblados
gbm,          # Boosting
stringi,      # Manipulación de texto
sf,           # Datos espaciales
tidymodels,   # Framework de modelado
spatialsample,# Muestreo espacial
plotly,       # Gráficos interactivos
leaflet,      # Mapas interactivos
tmaptools,    # Herramientas espaciales
osmdata,      # Datos OpenStreetMap
tm,           # Text Mining
tidytext,     # Procesamiento de texto
stopwords,    # Stopwords multilingües
parsnip,      # Especificación de modelos
dials,        # Hiperparámetros
workflows,    # Pipelines
tune,         # Tuning de modelos
yardstick,    # Métricas
udpipe,       # Procesamiento NLP
stringr,      # Manipulación de cadenas
nnet,         # Redes neuronales simples
SuperLearner
)
# 1. Definir conjuntos
y_train <- train_split$price
## --- Creacion de las particiones de datos para probar ----
#LLamamos la base desde GitHub, para no correr todo
train <- read.csv("https://raw.githubusercontent.com/samelomo99/PS3_SM_MB_DL/refs/heads/main/stores/trainfull.csv")
pacman::p_load(
readr,        # Importar datos
labelled,     # Manejo de etiquetas
naniar,       # Visualizar datos faltantes
DataExplorer, # Gráficos de missing values
psych,        # Estadísticas descriptivas
rvest,        # Web scraping
rio,          # Importar/exportar datos
tidyverse,    # Conjunto de paquetes tidy data
skimr,        # Resumen de datos
visdat,       # Visualizar datos faltantes
corrplot,     # Gráficos de correlación
gridExtra,    # Organización de gráficos
MASS,         # Funciones estadísticas diversas
stargazer,    # Tablas para salida a TEX
chromote,     # Automatización de navegador
ggplot2,      # Gráficos
boot,         # Funciones de bootstrap
patchwork,    # Combinación de gráficos
caret,        # Evaluación de modelos
purrr,        # Funciones map_*
kableExtra,   # Opciones de tablas
dplyr,        # Manipulación de datos
summarytools, # Descriptivos enriquecidos
knitr,        # kable en LaTeX
xtable,       # Tablas en LaTeX
tidyr,        # Separar/unir columnas
gmodels,      # CrossTable y más
glmnet,       # Regularización Lasso/Ridge
ranger,       # Random Forest rápido
randomForest, # Random Forest clásico
Metrics,      # Métricas de evaluación
adabag,       # Bagging y boosting
rsample,      # Remuestreo
rpart,        # Árboles de decisión
rpart.plot,   # Gráficos para árboles
ipred,        # Modelos ensamblados
gbm,          # Boosting
stringi,      # Manipulación de texto
sf,           # Datos espaciales
tidymodels,   # Framework de modelado
spatialsample,# Muestreo espacial
plotly,       # Gráficos interactivos
leaflet,      # Mapas interactivos
tmaptools,    # Herramientas espaciales
osmdata,      # Datos OpenStreetMap
tm,           # Text Mining
tidytext,     # Procesamiento de texto
stopwords,    # Stopwords multilingües
parsnip,      # Especificación de modelos
dials,        # Hiperparámetros
workflows,    # Pipelines
tune,         # Tuning de modelos
yardstick,    # Métricas
udpipe,       # Procesamiento NLP
stringr,      # Manipulación de cadenas
nnet,         # Redes neuronales simples
SuperLearner
)
## --- Creacion de las particiones de datos para probar ----
#LLamamos la base desde GitHub, para no correr todo
train <- read.csv("https://raw.githubusercontent.com/samelomo99/PS3_SM_MB_DL/refs/heads/main/stores/trainfull.csv")
test <- read.csv("https://raw.githubusercontent.com/samelomo99/PS3_SM_MB_DL/refs/heads/main/stores/testfull.csv")
# Creamos índices para dividir
index <- createDataPartition(train$price, p = 0.7, list = FALSE)
train_split <- train[index, ]  #Con esta se hace la estimación
test_split  <- train[-index, ] #Con esta se hace la prueba del MAE
# 1. Definir conjuntos
y_train <- train_split$price
x_train <- train_split %>% select(-price)
y_test <- test_split$price
x_test <- test_split %>% select(-price)
# 2. Entrenar SuperLearner
set.seed(123)
sl_fit <- SuperLearner(
Y = y_train,
X = x_train,
family = gaussian(),
SL.library = c("SL.mean", "SL.glm", "SL.rpart"),
method = "method.NNLS"
)
rm(list = ls())
require(pacman)
p_load(
tidyverse,   # Conjunto de paquetes para manipulaci?n y visualizaci?n de datos (dplyr, ggplot2, etc.)
rio,         # Importa y exporta datos en m?ltiples formatos (CSV, Excel, RDS, etc.)
osmdata,     # Descarga datos geoespaciales desde OpenStreetMap
sf,          # Trabaja con datos espaciales bajo el est?ndar "Simple Features"
SpatialKDE,  # Permite hacer estimaciones de densidad de kernel (KDE) espaciales
tmap,        # Visualizaci?n de datos espaciales: mapas est?ticos o interactivos
gridExtra,   # Permite organizar m?ltiples gr?ficos en una misma figura
evmix,       # Modelos de colas y extremos en distribuciones estad?sticas
units        # Manejo de unidades f?sicas (km, m?, etc.)
)
# Directorio
setwd("C:/Users/samel/OneDrive/Datos adjuntos/Universidad de los Andes/Tesis")
#data <- import("data/em2021.csv")
data_med <- import("data/med_ecv_2024.xlsx")
View(data_med)
